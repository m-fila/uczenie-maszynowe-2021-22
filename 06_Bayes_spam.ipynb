{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/m-fila/uczenie-maszynowe-2021-22/blob/main/06_Bayes_spam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNDQCDSFVdQ4"
   },
   "source": [
    "# **SPAM vs. naiwny klasyfikator Bayesa**\n",
    "Autor: Anna Dawid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seqEdVR1VdQ7"
   },
   "source": [
    "## Wprowadzenie\n",
    "Nigeryjski książę wciąż zarabia na użytkownikach elektronicznych skrzynek pocztowych ponad 700 tys. dolarów rocznie ([źródło](https://www.cnbc.com/2019/04/18/nigerian-prince-scams-still-rake-in-over-700000-dollars-a-year.html))! Jak to możliwe?\n",
    "\n",
    "Pierwsza przyczyna jest natury psychologicznej. Ofiary są poddawane \"perfekcyjnej burzy pokuszeń\", jak ujął to psycholog w wywiadzie, do którego linka dałam Wam powyżej. Spammerzy łączą granie na ludzkiej chciwości, ale także na pragnieniu bycia bohaterem. W końcu kto nie chciałby zarobić na byciu wspaniałomyślnym i szczodrym? W tej kwestii możemy pracować wyłącznie nad sobą.\n",
    "\n",
    "Możemy za to pracować nad filtrami antyspamowymi. Użyjemy techniki, która nazywa się \"worek ze słowami\" (bag of words) w połączeniu z naiwnym klasyfikatorem Bayesa. Choć to prosty klasyfikator, z powodzeniem jest używany współcześnie (np. [SpamAssassin](https://cwiki.apache.org/confluence/display/spamassassin/BayesInSpamAssassin)).\n",
    "\n",
    "Notebook oparty na tutorialach:\n",
    "*   https://towardsdatascience.com/spam-classifier-in-python-from-scratch-27a98ddd8e73\n",
    "*   https://towardsdatascience.com/spam-filtering-using-naive-bayes-98a341224038\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZJI7KHjVdQ7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZJI7KHjVdQ7"
   },
   "source": [
    "## Import danych treningowych\n",
    "https://www.kaggle.com/uciml/sms-spam-collection-dataset\n",
    "\n",
    "To dane przygotowane przez Almeida et al. na podstawie forum brytyjskiego, gdzie użytkownicy skarżą się na spamowe SMSy. Każdy wiersz składa się z kolumny opisującej czy wiadomość jest spamem, czy nie ('spam' czy 'ham'), a druga zawiera treść wiadomości."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x110k8pa-KD8"
   },
   "source": [
    "Proszę pobrać repozytorium ```uczenie-maszynowe-2021-22``` z serwisu github i uaktualnić ścieżkę do danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDclCxP2hhoJ"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/m-fila/uczenie-maszynowe-2021-22\n",
    "folder = 'uczenie_maszynowe-2021-22/dane/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 786,
     "status": "ok",
     "timestamp": 1606307159888,
     "user": {
      "displayName": "Anna Dawid",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgUQAwZ7wyayL4BbiM0n_EANCgBjSdZ9H14lgcCFEE=s64",
      "userId": "02862484648310443813"
     },
     "user_tz": -60
    },
    "id": "cWBDlNPHVdQ9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(...)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "054a8tEtDWw2"
   },
   "source": [
    "Dane zawierają zbyteczne kolumny. Proszę:\n",
    "* usunać kolumny zawierająca wartości \"NaN\"\n",
    "* zmienić nazwy kolumn \"v1\" i \"v2\" na \"label\", \"text\"\n",
    "\n",
    "**Wskazówka**: proszę użyć metod ```DataFrame.drop()``` oraz ```DataFrame.rename()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "054a8tEtDWw2"
   },
   "outputs": [],
   "source": [
    "df = df....\n",
    "df = df....\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xr86NzYAVdRF"
   },
   "source": [
    "Proszę wypisać na ekran treść maila o indeksie **57**\n",
    "\n",
    "**Wskazówka**: Dostęp do danych o zadanych indeksach można uzyskać przez [`Dataframe.loc`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1022,
     "status": "ok",
     "timestamp": 1606307160151,
     "user": {
      "displayName": "Anna Dawid",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgUQAwZ7wyayL4BbiM0n_EANCgBjSdZ9H14lgcCFEE=s64",
      "userId": "02862484648310443813"
     },
     "user_tz": -60
    },
    "id": "kBsb284aVdRG"
   },
   "outputs": [],
   "source": [
    "index = 57\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdn7hSzUVdRJ"
   },
   "source": [
    "Treść wiadomości:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wLflQN1VdRP"
   },
   "source": [
    "Proszę wypisać na ekran liczebność danych, czyli liczbę maili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdn7hSzUVdRJ"
   },
   "outputs": [],
   "source": [
    "print(\"Data contains {} emails.\".format(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFpUrBiRgFiV"
   },
   "source": [
    "## Analiza częstości występowania słów w obu klasach za pomocą biblioteki WordCloud\n",
    "\n",
    "To biblioteka pozwalająca generować śliczne obrazki, na których wielkość słów odpowiada częstości jego występowania w danym zbiorze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOUTxbowVdRM"
   },
   "outputs": [],
   "source": [
    "!pip3 install wordcloud\n",
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 972
    },
    "executionInfo": {
     "elapsed": 7261,
     "status": "ok",
     "timestamp": 1606307166419,
     "user": {
      "displayName": "Anna Dawid",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgUQAwZ7wyayL4BbiM0n_EANCgBjSdZ9H14lgcCFEE=s64",
      "userId": "02862484648310443813"
     },
     "user_tz": -60
    },
    "id": "L28LyPkEivX2",
    "outputId": "03d6e3d5-ca6a-4081-d0af-b8ce397a32f7"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# najpierw słowa ze spamu\n",
    "spam_words = \" \".join(list(df [df['label']=='spam']['text'] ))\n",
    "spam_plot = WordCloud(width = 512, height = 512).generate(spam_words)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(spam_plot);\n",
    "\n",
    "# teraz słowa z normalnych wiadomosci\n",
    "ham_words = \" \".join(list(df [df['label']=='ham']['text'] ))\n",
    "ham_plot = WordCloud(width = 512, height = 512).generate(ham_words)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(ham_plot);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxelOYTFVdRR"
   },
   "source": [
    "Dane w tej chwili są w postaci ciągów słów. Zamienimy je na postać numeryczną używająć algorymtu ```CountVectorizer```.\n",
    "Na początek zróbmy to dla prostego tekstu by zrozumieć jak działa ten algorytm. Proszę:\n",
    "* zaimportować bibliotekę zawierającą algorytm:\n",
    "\n",
    "```\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "```\n",
    "* używając metody ```CountVectorizer.fit(...)``` przeprowadzić trening algorytmu na zdaniach \n",
    "\n",
    "```\n",
    "[\"Ala ma kota.\", \"Kot? Kot ma wszy.]\n",
    "```\n",
    "* wypisać na ekran słownik utworzony przez algorytm ```CountVectorizer```: ```CountVectorizer.vocabulary_```\n",
    "* wypisać na ekran listę znalezionych słów: ```CountVectorizer.get_feature_names()```\n",
    "* przepowadzić transformację zdania do postaci numerycznej: ```CountVectorizer.transform(...)```\n",
    "* przeprowadzić transformację odwrotną:```vectorizer.inverse_transform(...)```\n",
    "* wypisać na ekran wszystkie reprezentacje zdania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ...\n",
    "text = ...\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(text)\n",
    "print(\"Vocabulary:\",...)\n",
    "print(\"Lista słów:\",...)\n",
    "text_transformed = vectorizer....\n",
    "print(\"Original text:\",text)\n",
    "print(\"Transformed text:\",text_transformed)\n",
    "print(\"Transformed text after decoding\",...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proszę:\n",
    "\n",
    "* przeprowadzić procedurę treningu i transformacji dla danych z e-maili.\n",
    "* proszę wypisać na ekran postać oryginalną i po transformacji maila o indeksie **57**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7254,
     "status": "ok",
     "timestamp": 1606307166421,
     "user": {
      "displayName": "Anna Dawid",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgUQAwZ7wyayL4BbiM0n_EANCgBjSdZ9H14lgcCFEE=s64",
      "userId": "02862484648310443813"
     },
     "user_tz": -60
    },
    "id": "WvM-XCWXVdRV"
   },
   "outputs": [],
   "source": [
    "vectorizer = ...\n",
    "vectorizer....\n",
    "text = ...\n",
    "text_transformed = ...\n",
    "\n",
    "print(...)\n",
    "print(...)\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHsTCZS6VdRg"
   },
   "source": [
    "## Trening klasyfikatora\n",
    "\n",
    "Proszę:\n",
    "* podzielić dane na część uczącą i treningową w stosunku **7:3**\n",
    "* wytrenować klasyfikator mail korzystając z naiwnego algorytmu Bayesa opartego o rozkład wielomianowy: ```MultinomialNB```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxelOYTFVdRR"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(...)\n",
    "\n",
    "# zaimportuj odpowiednią bibliotekę\n",
    "from sklearn.naive_bayes import ...\n",
    "# stwórz obiekt klasyfikatora\n",
    "model = ...\n",
    "# naucz klasyfikator na zbiorze uczącym\n",
    "model...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kWchjUPVdRn"
   },
   "source": [
    "## Ocena jakości\n",
    "\n",
    "Korzystając z funkcji napisanych na poprzednich ćwiczeniach:\n",
    "\n",
    "* wykonać predykcję na danych testowych\n",
    "* wypisać na ekran wartości metryk oraz macierz pomyłek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gsnhFFc9VdRU"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "def printScores(model, X, Y):\n",
    "    ...\n",
    "\n",
    "\n",
    "printScores(model, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXqzqM0cVdRs"
   },
   "source": [
    "## Analiza modelu\n",
    "\n",
    "Sprawdźmy, czego właściwie maszyna się nauczyła. Analizując współczynniki modelu proszę wskazać słowa które są istotne dkla klasyfikacji.\n",
    "\n",
    "Proszę:\n",
    "* wypisać na ekran współczynniki przypisane do kolejnych słów: ```MultinomialNB.coef_[0]```\n",
    "* stworzyć listę zawierającą indeksy posortowanych współczynników: ```np.argsort(...)```\n",
    "* wypisać na ekran po 10 słów o największych i najmniejszych wartościach współczynników.\n",
    "\n",
    "**Wskazówka** by listę słów móc adresować listą indeksów, listę słów trzeba zamienić na macierz numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMnNeNTdVdRX"
   },
   "outputs": [],
   "source": [
    "# np.argsort zwraca indeksy w oryginalnej tablicy, które odpowiadają posortowanej tablicy, np.:\n",
    "# np.argsort([3,1,2]) ---> [1,2,0]\n",
    "# .coef_[0] zwraca nam tablicę współczynników -- po jednym dla każdej cechy. \n",
    "# Wkład do decyzji klasyfikatora jest proporcjonalny do wartości tych współczynników -- większy współczynnik = ważniejsza cecha\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "feature_names = np.array(feature_names)\n",
    "coeff = model.coef_[0]\n",
    "top10 = np.argsort(coeff)[-10:]\n",
    "bottom10 = np.argsort(coeff)[:10]\n",
    "\n",
    "print(\"Słowa, które z największą pewnością wskazują maszynie, że wiadomość to spam:\")\n",
    "print(feature_names[top10])\n",
    "\n",
    "print(\"Słowa najmniej istotne przy klasyfikacji:\")\n",
    "print(feature_names[bottom10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJv9ICOnkEqB"
   },
   "source": [
    "## Działanie modelu na nieznanych słowach.\n",
    "\n",
    "Model jedynie rozpoznaje poszczególne słowa, bez żadnej analizy językowej. Dodamy pewien element analizy językowej.\n",
    "Proszę:\n",
    "\n",
    "* wybrać dowolne, bezsensowne słowo które nie występuje w słowniku i sprawdzić, że tak jest\n",
    "* dokończyć zdanie : ``Life is`` tym słowem\n",
    "* wypisać postać przetransformowaną, oraz jej transformację odwrotną\n",
    "* sprawdzić, jak na nie zareaguje model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTx3RGrsVdRZ"
   },
   "outputs": [],
   "source": [
    "word = \"...\n",
    "index = np.where(feature_names == word) \n",
    "print(\"Indeks słowa {} to: {}\".format(word, index))\n",
    "\n",
    "message = ...\n",
    "message_transformed = ...\n",
    "\n",
    "print(\"Original text:\",message)\n",
    "print(...)\n",
    "print(...)\n",
    "           \n",
    "result = ...\n",
    "print(\"Model result for senstence: \\n {} is {}\".format(message,result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCgizjFcmQOr"
   },
   "source": [
    "### Stemming (bez tłumaczenia na polski, to [bogate](https://pl.bab.la/slownik/angielski-polski/stemming) znaczeniowo słowo)\n",
    "\n",
    "Polega na ujednoliceniu słów o tym samym rdzeniu znaczeniowym (o czym maszyna, oczywiście, nie ma szans wiedzieć). Np. dzięki stemmingowi słowa \"go\", \"going\" i \"goes\" są przyporządkowane tylko jednemu słowu \"go\". Można np. użyć gotowego algorytmu stemmingowego o nazwie [Porter Stemmer](https://tartarus.org/martin/PorterStemmer/).\n",
    "\n",
    "Proszę:\n",
    "\n",
    "* wypisać na ekran indeksy słów ``going`` oraz ``go``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCxWJnnSVdRd"
   },
   "outputs": [],
   "source": [
    "id1 = np.where(feature_names == 'going')\n",
    "id2 = ...\n",
    "print(\"Indices for words going and go are: \"...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1946,
     "status": "ok",
     "timestamp": 1606307432527,
     "user": {
      "displayName": "Anna Dawid",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgUQAwZ7wyayL4BbiM0n_EANCgBjSdZ9H14lgcCFEE=s64",
      "userId": "02862484648310443813"
     },
     "user_tz": -60
    },
    "id": "Y3tQCtFu8JEU",
    "outputId": "759237e2-4d84-4099-be50-06f4577e21b5"
   },
   "outputs": [],
   "source": [
    "!pip3 install nltk\n",
    "import nltk\n",
    "import ssl\n",
    "# chcemy pobrac wytrenowany dla języka angielskeigo tokenizer Punkt\n",
    "# poniższe linijki mają pomóc w przypadku problemów z ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "# pobieramy tokenizer\n",
    "nltk.download('punkt')\n",
    "# ładujemy przydatne funkcje\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHsTCZS6VdRg"
   },
   "outputs": [],
   "source": [
    "# przykładowy tekst\n",
    "message = 'Applying classical methods of machine learning to the study of quantum systems (sometimes called quantum machine learning) is the focus of an emergent area of physics research'\n",
    "# tokenizujemy czyli dzielimy tekst na słowa\n",
    "words = word_tokenize(message)\n",
    "\n",
    "print(\"Message:\\n\",message)\n",
    "print(\"tokens:\\n\",words)\n",
    "\n",
    "# stwórz obiekt typu PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "# użyj nowo stworzonego obiektu, żeby 'dostać temat' każdego ze słów\n",
    "words = [stemmer.stem(word) for word in words]\n",
    "print(\"words stems:\\n\",words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRkUybiA9oit"
   },
   "source": [
    "Powtórzmy trening i testowanie naszego klasyfikatora na danych poddanych stemmingowi. Proszę:\n",
    "\n",
    "* stworzyć zmienną zawierająca kolumnę treści maili\n",
    "* poddać ją transformacji z użyciem klasy ```PorterStemmer```\n",
    "* podzielić dane na części uczącą i testową\n",
    "* następnie wytrenować i poddać transformacji za pomocą ```CountVectorizer```\n",
    "* następnie wytrenować model i wypisać dla niego wartości metryk\n",
    "\n",
    "**Wskazówka** do transformacji kolumny słów użyć linii, które zdefiniowano zmienną ```words``` w komórce powyżej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wracamy do pracy nad zbiorem maili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "executionInfo": {
     "elapsed": 2025,
     "status": "ok",
     "timestamp": 1573803469436,
     "user": {
      "displayName": "Anna Dawid",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBMAlqIzrWbyBbGSDvCFuCvvSN7Xx3h3HRKaToc0r4=s64",
      "userId": "02862484648310443813"
     },
     "user_tz": -60
    },
    "id": "asGIjB4J-Jrq",
    "outputId": "b135bff3-c46f-4354-e762-bc88549f6419"
   },
   "outputs": [],
   "source": [
    "# stwórz instancje obiektu CountVectorizer\n",
    "vectorizer = ...\n",
    "\n",
    "#przeprowadź serię transformacji\n",
    "text = ...\n",
    "text_stemmed = ...\n",
    "text_transformed = ...\n",
    "\n",
    "#przeprowadź podział na dane uczące i treningowe\n",
    "X_train, X_test, Y_train, Y_test = ...\n",
    "\n",
    "# stwórz obiekt klasyfikatora\n",
    "model = ...\n",
    "\n",
    "# naucz klasyfikator na zbiorze uczącym\n",
    "...\n",
    "\n",
    "# wypisz wartości metryk\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WxLEc7CASHi"
   },
   "source": [
    "## Gdybyście byli spammerami... Co moglibyście zrobić, znając tę technikę antyspamową?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOLZbs_5AY7Z"
   },
   "source": [
    "### Stosowanie znaków specjalnych zamiast liter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 502,
     "status": "ok",
     "timestamp": 1573803724823,
     "user": {
      "displayName": "Anna Dawid",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBMAlqIzrWbyBbGSDvCFuCvvSN7Xx3h3HRKaToc0r4=s64",
      "userId": "02862484648310443813"
     },
     "user_tz": -60
    },
    "id": "qccAaFGY_C8Y",
    "outputId": "8e1a5000-2681-457d-b3cc-46d0e8a9e2a0"
   },
   "outputs": [],
   "source": [
    "our_message = vectorizer.transform(['call for free'])\n",
    "print(model.predict(our_message))\n",
    "# podmieniamy literkę a na małpę @\n",
    "our_tricky_message = vectorizer.transform([...])\n",
    "print(model.predict(our_tricky_message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbYzjoYFBDs-"
   },
   "source": [
    "Jakieś inne pomysły? :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1573803857165,
     "user": {
      "displayName": "Anna Dawid",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBMAlqIzrWbyBbGSDvCFuCvvSN7Xx3h3HRKaToc0r4=s64",
      "userId": "02862484648310443813"
     },
     "user_tz": -60
    },
    "id": "NIAyQbAtBJbI",
    "outputId": "acf18768-ea2b-499b-b8d6-2ae78dcfb1a1"
   },
   "outputs": [],
   "source": [
    "our_tricky_message = vectorizer.transform(['Call for free sex otherwise you miss a very important meeting'])\n",
    "print(clf.predict(our_tricky_message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dlaczego powyższa wiadomość została sklasyfikowana jako pożądana, chociaż jest ewidentnym przykładem spamu?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "05M_Bayes_spam.ipynb",
   "provenance": [
    {
     "file_id": "1BY9TSukNLV3VgtNdHpcgWofeAtVF4yku",
     "timestamp": 1573037493591
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
