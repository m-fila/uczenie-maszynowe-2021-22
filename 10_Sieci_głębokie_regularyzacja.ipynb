{
 "cells": [
 {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/m-fila/uczenie-maszynowe-2021-22/blob/main/10_Sieci_głębokie_regularyzacja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTxi8m8GEfHL"
   },
   "source": [
    "# Sieci neuronowe \r\n",
    "Autor: Anna Dawi\n",
    "\n",
    "Korekta: A. Kalinowskid\r\n",
    "\r\n",
    "Dzisiaj pod lupę weźmiemy jeden z najpopularniejszych i wszechstronnych modeli uczenia maszynowego - sieci neuronowe. Skupimy się na dwóch zagadnieniach: jak znaleźć optymalną architekturę sieci oraz o arcyważnym problemie regularyzacji. Będziemy pracować na zbiorze MNIST, czyli zbiorze czarno-białych obrazków z ręcznie napisanymi cyframi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie środowiska programistycznego\n",
    "\n",
    "By zapewnić powtarzalność wyników ustawiany ziarno generatora liczb losowych:\n",
    "```\n",
    "seed = 128\n",
    "rng = np.random.RandomState(seed)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "from termcolor import colored\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 128\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "def testModelOnMyDigits(model):    \n",
    "    #Code created by M.Fila\n",
    "    if not os.path.isdir(\"colab_freehands\"):\n",
    "        !git clone https://github.com/m-fila/colab_freehands.git\n",
    "\n",
    "    from colab_freehands.canvas import Canvas  \n",
    "    canvas = Canvas(line_width=2)\n",
    "    example = (\n",
    "        canvas.to_array(size=(20, 20), margin=(4, 4), dtype=np.float32, weighted=True) / 255\n",
    "    )\n",
    "    example_flatten = example.reshape( (-1))\n",
    "    predictions = model(np.expand_dims(example_flatten, (0, -1)))\n",
    "    plt.imshow(example, cmap=\"gray\")\n",
    "    plt.show()\n",
    "    print(\n",
    "        \"Predicted class: {} ({:.0f}%)\".format(\n",
    "            np.argmax(predictions), np.max(predictions) * 100\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1bbElA8FMVw"
   },
   "source": [
    "## Import danych MNIST\n",
    "\n",
    "Proszę:\n",
    "\n",
    "* wczytać dane korzysatając z funkcji ```keras.datasets.mnist.load_data()```\n",
    "* wypisać na ekran kształ danych uczących i testowych. Ile jest przykładów uczących i testowych? Jaką rozdzielczośc mają analizowane rysunki?\n",
    "* korzystając z funkcji ```matplotlib.pyplot.imshow()``` narysować przykład numer 0 z danych uczących i wypisać jego etykietę"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "print(colored(\"Training data features:\", \"blue\"), ...)\n",
    "print(colored(\"Training data labels:\", \"blue\"), ...)\n",
    "\n",
    "print(colored(\"Test data features:\", \"blue\"), ...)\n",
    "print(colored(\"Test data labels:\", \"blue\"), ...)\n",
    "\n",
    "print(colored(\"Trainig data example number 0:\", \"blue\"), \"label:\", ...)\n",
    "print(...)\n",
    "\n",
    "plt.imshow(..., cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wstępne przygotowanie danych (ang. preprocessing)\n",
    "\n",
    "Dane które analizujemy mają postać dwuwymiartowych macierzy, ale sieci neuronowe które dziś trenujemy przyjmują na wejściu jednowymiarowe wektory.\n",
    "Proszę:\n",
    "\n",
    "* zmienić kształ danych wejścionych na jednowymiarowe macierze - operacja \"spłaszczenia\"\n",
    "* wypisać kształ macierzy po spłaszeniu. Czy wymiar jest zgodny z oczekiwaniem?\n",
    "* znormalizować wartości danych do zakresu **[0,1]** korzysatając z funkcji ```numpy.amax(...)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data shape before flattening:\", ...)\n",
    "\n",
    "X_train = ...\n",
    "X_test = ...\n",
    "\n",
    "print(\"Training data shape after flattening:\",...)\n",
    "\n",
    "maxValue = ...\n",
    "print(\"Maximum value in training data:\",maxValue)\n",
    "\n",
    "X_train = ...\n",
    "X_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zmiana reprezentacji etykiet\n",
    "\n",
    "Etykiety zawierają numer klasy - cyfry. Łatwiejszą do analizy postacią jest reprezentacja za pomocą słowa bitowego o długości równej licznie klas.\n",
    "W takim słowie wszystkie bity, oprócz jednego - wsazującego na daną klasę mają wartość **0**:\n",
    "\n",
    "```\n",
    "Original label encoding: 5 shape: (60000,)\n",
    "One hot label encoding: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] shape: (60000, 10)\n",
    "\n",
    "```\n",
    "\n",
    "Takie kodowanie nazywa się \"one hot encoding\".\n",
    "\n",
    "Proszę:\n",
    "\n",
    "* korzystając z funkcji ```tensorflow.one_hot(...)``` zamienić etykiety na reprezentację \"one hot encoding\"\n",
    "* wypisać na ekran orginalne i nowe kodowanie etykiety dla przykładu 0 ze zbioru uczącego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original label encoding:\",Y_train[0], \"shape:\", Y_train.shape)\n",
    "depth = 10\n",
    "\n",
    "Y_train = tf.one_hot(Y_train, depth)\n",
    "Y_test = ...\n",
    "\n",
    "print(\"One hot label encoding for training data:\",..., \"shape:\", ...)\n",
    "print(\"One hot label encoding for test data:\",..., \"shape:\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicja architektury sieci neuronowej\n",
    "\n",
    "Proszę zdefiniować sieć neuronową o architekturze w pełni połączonej (ang. fully connected). Sieć powinna mieć:\n",
    "* warstwę wejściową\n",
    "* 4 warstwy ukryte o 500 neuronach każda z funkcją aktywacji ```relu``\n",
    "* wartę wyjściową z funkcją aktywacji ```sofmax```  \n",
    "\n",
    "\n",
    "Proszę:\n",
    "\n",
    "* przeanalizować funkcję ```getModel(nHidden, nNeurons, inputWidth, outputWidth)``` która tworzy model o zadanej liczbe wartw ukrytych ```nHidden``` z zadaną liczbą neuronów w każdej warstwie ```nNeurons``` przyjmujący na wejściu wektor o długości ```inputWidth```\n",
    "* obliczyć samodzielnie liczbę parametrów pierwszej warsty ukrytej i porównać ją z wynikiem działania funkcji ```model.summary(...)```\n",
    "* obejrzeć rysunek przedstawiający architekturę modelu, uzysdkany przy pomocy funkcji ```tf.keras.utils.plot_model(...)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(nNeurons, nHiddenLayers, inputWidth, outputWidth):\n",
    "      \n",
    "    inputs = tf.keras.Input(shape=(inputWidth,))\n",
    "    x = inputs\n",
    "    for iHidden in range(nHiddenLayers):   \n",
    "        x = tf.keras.layers.Dense(nNeurons, activation=tf.nn.relu)(x)\n",
    "  \n",
    "    outputs = tf.keras.layers.Dense(outputWidth, activation=tf.nn.softmax)(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "nNeurons = 500\n",
    "nHiddenLayers = 5\n",
    "inputWidth = 28*28\n",
    "outputWidth = 10\n",
    "model_basic = getModel(nNeurons, nHiddenLayers, inputWidth, outputWidth)\n",
    "\n",
    "print(colored(\"Number of parameters for the first hidden layer:\",\"blue\"),inputWidth*nNeurons+nNeurons)\n",
    "\n",
    "model_basic.summary()\n",
    "tf.keras.utils.plot_model(model_basic, 'ML_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trenowanie modelu\n",
    "\n",
    "Trenowanie modelu polega na znalezienu parametrów modelu dla których funkcja straty przyjmuje minimalną wartość. Problem ten w ogólności nie może być rozwiązany analiztycznie i minimalizacja jest przeprowadzana numerycznie. Standardowym wyborem algorytmu minimalizacji jest \n",
    "[adaptive momemtum estimation](https://arxiv.org/abs/1412.6980). W sytuaccji kiedy model ma za zadanie kategoryzację danych jako funkcję straty przyjmuje się zwykle entropię krzyżową (ang. crossentropy). W sytuacji kiedy klas jest więcej niż dwie trzeba użyć wariantu \"categorical_crossentropy\".\n",
    "\n",
    "\n",
    "Trening jest prowadzony iteracyjnie. Każde przejście przez pełen zestaw danych jest **epoką**. Dane zwykle są podzielone na paczki **batch**.\n",
    "Postęp treningu można monitorować używająć wybranej metryki na danych uczących i testowych. Zwykle używa się dokładności.\n",
    "\n",
    "Proszę:\n",
    "\n",
    "* przeprowadzić trening dla `10` epok z rozmiarem paczki wynoszącym `128`\n",
    "* jako algorytmu minimalizacji proszę użyć `adam`\n",
    "* jako funkcji straty proszę użyć `categorical_crossentropy`\n",
    "* jako metryki proszę użyć `accuracy`\n",
    "\n",
    "**Uwaga**: po pierwszym treningu proszę pełączyć śrdowisko wykonawcze by używało karty graficznej \"GPU\": z menu na górze:\n",
    "```\n",
    "Środowisko wykonawcze -> Zmień typ środowiska wykonawczego\n",
    "```\n",
    "Oczekiwany efekt:\n",
    "```\n",
    "Epoch 1/10\n",
    "469/469 [==============================] - 2s 3ms/step - loss: 0.2286 - accuracy: 0.9302 - val_loss: 0.1268 - val_accuracy: 0.9619\n",
    "\n",
    "...\n",
    "\n",
    "Epoch 10/10\n",
    "469/469 [==============================] - 1s 2ms/step - loss: 0.0233 - accuracy: 0.9930 - val_loss: 0.0861 - val_accuracy: 0.9803\n",
    "```\n",
    "\n",
    "Proszę:\n",
    "* porównać wartości metryki na danych uczących i testowych. Jaki wniosek wynika z tego porównania?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "epochs = 15\n",
    "batch_size = 128\n",
    "\n",
    "model_basic_fit = model_basic.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza historii treningu\n",
    "\n",
    "Proszę uzupełnić funkcję ```plotTrainingHistory(model)``` tak by tworzyła wykresy:\n",
    "\n",
    "* na jednym wykresie wartości metryki w funkcji numeru epoki obliczone dla danych uczących i treningowych\n",
    "* na drugim wykresie funkcji straty  w zależności od numeru epoki obliczone dla danych uczących i treningowych\n",
    "\n",
    "Wartości potrzebnych parametrów są dostępne w obiekcie ```Model.history```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_basic_fit.history.keys())\n",
    "history = model_basic_fit.history\n",
    "print(history['accuracy'])\n",
    "\n",
    "def plotTrainingHistory(model):\n",
    "\n",
    "    fig, axes= plt.subplots(1,2,figsize=(10,5))\n",
    "    history = model.history\n",
    "    axes[0].plot(...)\n",
    "    axes[0].plot(...)\n",
    "    axes[0].set_ylabel(...)\n",
    "    axes[0].set_xlabel(...)\n",
    "    axes[0].legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "    axes[1]...\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "    \n",
    "plotTrainingHistory(model_basic_fit)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprawdzenie modelu na własnych danych\n",
    "\n",
    "Proszę uruchomić komórkę poniżej i sprawdzić jak model rozpoznaje własnoręcznie napisane cyfry\n",
    "\n",
    "**Uwaga** ten fragment działa tylko w Google Colaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB: \n",
    "    testModelOnMyDigits(model_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularyzacja procesu treningu\n",
    "\n",
    "Analiza historii uczenia i wartości metryk na zbiorahc uczącym i treninsgowym wskazuje, że uzyskany model nie generalizuje się dobrze - jego odpowiedź na dane których nie \"widział\" w czasie treningu jest gorsza niż na dane uczące. Jednym ze sposobów redukcji tego efektu jest **regularyzacja** modelu poprzez nałożeniu \n",
    "różnych ograqniczeń na proces uczenia to mogą być:\n",
    "\n",
    "* zmiana liczby epok i zatrzymanie uczenia w chwili gdy metryka na zbiorze testowym się nie zmienia, lub pogarsza - ang. **early stopping**\n",
    "* ograniczenia na wartości wag uzyskane przez dodawanie członów do funkcji straty kierujących prosec uczenia to wybranego obszaru wag, np. małych wartości wag - **L1 or L2 penalty term**\n",
    "* losowe wyłączanie neuronów - ang. **dropout**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping\n",
    "\n",
    "Proszę:\n",
    "\n",
    "* stworzyć model ```model_early_stop``` o tej samej architekturze co model ```model_basic```\n",
    "* przeprowadzić trening z opcją \"early_stop\": ```callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)]``` Co oznacza parametr \"cierpliwość\"?\n",
    "* narysować rysunki dla historii treningu\n",
    "\n",
    "Czy moment przerwania treningu jest zgodny z oczekiwaniem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_early_stop = ...\n",
    "\n",
    "model_early_stop_fit = model_early_stop.fit(...)\n",
    "\n",
    "plotTrainingHistory(...) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularyzacja L2 i L1\n",
    "\n",
    "Regularyzacje L1 i L2 prowadzą do uzyskania moeli które mają małe wartości wag. Efekty ten uzyskuje się przez dodanie członu \"kary\" ang. \"penalty term\" do funkcji straty:\n",
    " \n",
    "\\begin{equation}\n",
    "L_{1} = \\lambda \\sum_{i} w_{i},~~\n",
    "L_{2} = \\lambda \\sum_{i} w_{i}^{2}\n",
    "\\end{equation}\n",
    "\n",
    "gdzie $w_{i}$ to wagi modelu, a $\\lambda$ to parametr skalujący wielkość członu kary.\n",
    "\n",
    "Proszę:\n",
    "\n",
    "* stworzyć model ```model_L2``` o tej samej architekturze co model ```model_basic``` z regularyzacją L2 dla każdej warstwy. Efekt ten możne uzyskać na conajmniej dwa sposoby:\n",
    "    * napisać nową funkcję ```getModelWithL2(...)``` z opcją regularyzacji L2 dla każdej warstwy: ```kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)```\n",
    "    * dodać regularyzację dla warstw istniejącego modelu:\n",
    "    \n",
    "      ```\n",
    "      layers = model_basic.layers\n",
    "      ...\n",
    "      layer.kernel_regularizer =  tf.keras.regularizers.l2(l2_lambda)\n",
    "      ```\n",
    "      W tym wypadku należy skompilowac model powtórnie:\n",
    "      ```\n",
    "      model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "      ```\n",
    "* przeprowadzić trening z parametrami jak poprzednio\n",
    "* narysować rysunki dla historii treningu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_lambda = 0.001\n",
    "\n",
    "model_L2 = ...\n",
    "\n",
    "...\n",
    " \n",
    "model_L2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_L2_fit = model_L2.fit(...)\n",
    "plotTrainingHistory(...)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Opuszczanie neuronów (ang. dropout) polega na dezaktywacji losowych neutronów w czasie treningu. Dezaktywacja neuronów jesty uzyskiwane przez zerowanie wagi danwego neuronu względem następnej wartwy. Efekt ten uzyskuje się przez wstawienie dedykowanerj warstwy - \"tf.keras.Dropout\" po warstwie w której mają być dezaktywowane neurony.\n",
    "\n",
    "Proszę:\n",
    "\n",
    "* napisać funkcję ```getModelWithDropout(nNeurons, nHiddenLayers, inputWidth, outputWidth, dropout_rate)``` która tworzy model z warstawmi \"Dropout\" po każdej warsdtwie ukrytej\n",
    "* stworzyć model ```model_droput``` o tej samej architekturze co model ```model_basic``` z warstwami \"Dropout\" po każdej warstwie ukrytej.\n",
    "* wypisać na ekran architekturę modelu, używając funkcji ```model.summary()``` i sprawdzić czy struktura modelu jest zgodna z oczekiwaniem\n",
    "* przeprowadzić trening z parametrami jak poprzednio\n",
    "* narysować rysunki dla historii treningu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelWithDropout(nNeurons, nHiddenLayers, inputWidth, outputWidth, dropout_rate):\n",
    "      \n",
    "    ...\n",
    "    ...\n",
    "    return model\n",
    "\n",
    "dropout_rate = 0.25\n",
    "model_dropout = ...\n",
    "model_dropout.summary()\n",
    "\n",
    "model_dropout_fit = model_dropout.fit(...)\n",
    "plotTrainingHistory(...) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Połączenie metod L2 i dropout\n",
    "\n",
    "Proszę:\n",
    "\n",
    "* napisać funkcję ```getModelWithDropoutWithL2(nNeurons, nHiddenLayers, inputWidth, outputWidth, dropout_rate, l2_lambda)``` która tworzy model z warstawmi \"Dropout\" po każdej warstwie ukrytej, oraz regularyzacją typu L2 o tej samej architekturze co model ```model_basic```\n",
    "* stworzyć model ```model_droput_L2```\n",
    "* wypisać na ekran architekturę modelu, używając funkcji ```model.summary()``` i sprawdzić czy struktura modelu jest zgodna z oczekiwaniem\n",
    "* przeprowadzić trening z parametrami jak poprzednio\n",
    "* narysować rysunki dla historii treningu. \n",
    "* **proszę przeanalizować i skomentować wykresy**. Czy są one zgodne wykresami dla innych modeli?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelWithDropoutWithL2(nNeurons, nHiddenLayers, inputWidth, outputWidth, dropout_rate, l2_lambda):\n",
    "      \n",
    "    ...\n",
    "    return model\n",
    "\n",
    "...\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walidacja modelu\n",
    "\n",
    "Który z wariantów jest optymalny? Czy mamy wystarczające dane by odpowiedzieć na to pytanie? \n",
    "\n",
    "Proszę:\n",
    "\n",
    "* korzystająć z funkcji ```sklearn.model_selection.train_test_split(...)``` podzielić zbiór uczący na nowy uczący i walidacyjny w stosunku 7:3\n",
    "* korzystając ze zbiorów uczącego i walidacyjnego wybrać optymalny model\n",
    "* przeprowadzić trening modeli: `basic`, `early_stop`, `L2` i `L2_dropout` sprawdzając wydajność modelu na zbiorze walidacyjnym. W czasie treningu można zmiejszyć ilośc wypisywanych danych używając parametru ```verbose=2```\n",
    "* wybrać model o najlepszej dokładności\n",
    "* sprawdzić dokładność wybranego modelu na zbiorze testowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ...\n",
    "\n",
    "#Reload the data\n",
    "...\n",
    "\n",
    "#Check the shapes\n",
    "...\n",
    "\n",
    "#Flatten the data\n",
    "...\n",
    "\n",
    "#Normalise the data\n",
    "...\n",
    "\n",
    "#Recode the labels\n",
    "...\n",
    "\n",
    "#Traing the models\n",
    "...\n",
    "\n",
    "print(colored(\"Best model:\",\"blue\"))\n",
    "...\n",
    "\n",
    "print(colored(\"Best model metric on test data:\",\"blue\"))\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 491436,
     "status": "ok",
     "timestamp": 1610535700814,
     "user": {
      "displayName": "Anna Dawid",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgUQAwZ7wyayL4BbiM0n_EANCgBjSdZ9H14lgcCFEE=s64",
      "userId": "02862484648310443813"
     },
     "user_tz": -60
    },
    "id": "tkn7pEjjX8ET"
   },
   "source": [
    "## Zadanie domowe\n",
    "\n",
    "### Warstawa spłaszczająca\n",
    "\n",
    "Pierwsza operacja wstępnej analizy danych polegała na spłaszczeniu wielowymiarowje struktury do jednowymiarowego wektora. To jest częsta operacja i zdefiniowano dla niej odpowiednią warstwę (ang. layer).\n",
    "\n",
    "Proszę:\n",
    "\n",
    "* korzystająć z warstwy spłasznapisać funkcję ```getModelFinal(...)``` która tworzy wszystkim elementami (regularyzacja L2, dropout) który na wejściu przyjmuje oryginalne rysunki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelFinal(nNeurons, nHiddenLayers, inputWidth, outputWidth, dropout_rate, l2_lambda):\n",
    "      \n",
    "    ...\n",
    "    return model\n",
    "\n",
    "dropout_rate = 0.25\n",
    "model_final = ...\n",
    "\n",
    "\n",
    "#Reload the data\n",
    "...\n",
    "\n",
    "#Normalise the data\n",
    "...\n",
    "\n",
    "#Recode the labels\n",
    "...\n",
    "\n",
    "\n",
    "model_final = model_final.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, Y_test), verbose=2)\n",
    "plotTrainingHistory(model_final) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNgN4Ns96O4PKYyYUFlC4En",
   "collapsed_sections": [],
   "name": "10M_Sieci_głębokie_regularyzacja.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
