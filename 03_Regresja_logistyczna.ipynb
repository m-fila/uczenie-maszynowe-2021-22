{
 "cells": [
   {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/m-fila/uczenie-maszynowe-2021-22/blob/main/03_Regresja_logistyczna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMv_I3EiF6Rd"
   },
   "source": [
    "# Regresja logistyczna, walidacja krzyżowa, krzywe ROC\n",
    "Autor: Jarosław Żygierewicz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJjklVp9F6Rg"
   },
   "source": [
    "## Część I: regresja logistyczna\n",
    "Ten notebook pomoże Ci zapoznać się z regresją logistyczną. \n",
    "\n",
    "Zbudujemy klasyfikator bazujący na regresji logistycznej. Jego zadaniem będzie określanie prawdopodobieństwa przyjęcia kandydata na studia na podstawie wyników z dwóch egzaminów maturalnych (każdy przeskalowany na zakres 0-100%): z matematyki i z biologii. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpPO3J51F6Rj"
   },
   "source": [
    "Zanim przejdziemy do właściwych zadań zaimportujmy potrzebne moduły:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zestaw uczący pobieramy z repozytorium github i wczytujemy używając klasy Pandas DataFrame. Najpierw pobieramy repozytorium z github. Repozytorium zawiera kod do naszych ćwiczeń, oraz przykładowe dane w katalogu \"dane\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odkomentuj zeby pobrac repozytorium, mozesz tez wgrac samemu odpowiedni plik z danymi\n",
    "#!git clone https://github.com/m-fila/uczenie-maszynowe-2021-22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"uczenie-maszynowe-2021-22/dane/reg_log_data.txt\", encoding='latin-1', sep=\",\", names=[\"matematyka\", \"biologia\", \"wynik\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tK8shSDdF6R2"
   },
   "source": [
    "Aby łatwiej było się nimi posługiwać wydzielmy z nich dane wejściowe jako 'X' i wyjściowe jako 'Y':"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzoi2OTNF6R7"
   },
   "source": [
    "## Analiza wizualna danych. \n",
    "\n",
    "Pierwszy krok przy analizie danych z użyciem dowolnego algorytmu to ich inspekcja. Korzystając z metod klasy DataFrame proszę:\n",
    "* wypisać na ekran framgent danych\n",
    "* narysować rozkłady wszystkich zmiennych wejściowych, w naszym przypadku wyniku egzaminów z matematyki i biologii dla całego zbioru\n",
    "* narysować rozkłady wszystkich zmiennych wejściowych, w naszym przypadku wyniku egzaminów z matematyki i biologii dla wierszy gdzie wynik=0\n",
    "* narysować rozkłady wszystkich zmiennych wejściowych, w naszym przypadku wyniku egzaminów z matematyki i biologii dla wierszy gdzie wynik=1\n",
    "\n",
    "**Wskazówka**: proszę użyć filtrowania danych, tak jak to było robione na pierwszych zajęciach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uzyj tego do okreslenia rozmiaru obrazka\n",
    "figsize=(15,5)\n",
    "# wypisz dataframe\n",
    "...\n",
    "# narysuj histogramy dla wszystkich kolumn\n",
    "...\n",
    "# wyplotuj tych co zdali\n",
    "...\n",
    "# wyplotuj tych co nie zdali\n",
    "...\n",
    "\n",
    "#To samo co powyżej w jednej linii\n",
    "df.hist(figsize=figsize, by=\"wynik\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To co nas interesuje to są jednak korelacje między zmiennymi. Korzystając z przykładu z pierwszych ćwiczeń proszę:\n",
    "\n",
    "* narysować wykres korelacji między zmiennymi wejściowymi dla pełnych danych, oraz wierszy gdzie wynik=0 lub 1\n",
    "\n",
    "**Wskazówka**: proszę użyć parametru \"hue\" funkcji sns.jointplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "x = sns.jointplot(...)\n",
    "x.set_axis_labels('matematyka', 'biologia', fontsize=16);\n",
    "\n",
    "x = sns.jointplot(...)\n",
    "x.set_axis_labels('matematyka', 'biologia', fontsize=16);\n",
    "\n",
    "x = sns.jointplot(...)\n",
    "x.set_axis_labels('matematyka', 'biologia', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCic0Ta2F6SH"
   },
   "source": [
    "## Hipoteza\n",
    "Dla przypomnienia _hipoteza_ w regresji logistycznej ma postać: \n",
    "\n",
    "$\\qquad$ $h_\\theta(x) = \\frac{1}{1+\\exp(-\\theta x^T )}$.\n",
    "\n",
    "W implementacji dobrze jest myśleć o tej funkcji tak:\n",
    "\n",
    "$\\qquad$ $h_\\theta(x) = \\frac{1}{1+f}$.\n",
    "\n",
    "gdzie: $f = \\exp(-\\theta x^T)$\n",
    "\n",
    "Proszę napisać funkcję ```logistic_func(x, theta)``` która:\n",
    "\n",
    "* implementuje funkcję logistyczną\n",
    "* jako argumenty przyjmuje parametry regresji logistycznej  $(\\theta_{0}, \\theta_{1}, ..., \\theta_{i})$ oraz tablicę danych wejściowych $x$. \n",
    "* w kodzie fukcji proszę rozszerzyć tablicę $x$ o dodatkową kolumnę jedynek, by parametr $\\theta_{0}$ był traktowany na tej same zasadzie co pozostałe parametry\n",
    "* ze względu na stabilność numeryczną obliczeń ma ograniczony zakres zmienności. Proszę ograniczyć wartości wykładnika w mianowniku do zakresu  $\\pm18$\n",
    "\n",
    "**Ostrzeżenie:** x to tablica która może zawierać wiele kolumn i wiele wierszy.\n",
    "\n",
    "**Wskazówka**: ograniczając zakres zwracanych wartości proszę skorzystać z funkcji np.where() zaaplikowanej do wektora wartości wykładnika.\n",
    "\n",
    "Proszę sprawdzić działanie funkcji na następujących danych testowych:\n",
    "```\n",
    "theta = np.array([1,1,2])\n",
    "x = np.array([[5,5],\n",
    "              [5,6],\n",
    "              [-5,-5],\n",
    "              [-5,-8]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_func(theta, x):\n",
    "    # dodaj kolumne jedynek\n",
    "    x_expanded = ...\n",
    "    # policz argument funkcji\n",
    "    arg = ...\n",
    "    # uzyj np.where żeby ograniczyc wartosci parmetru do [-18,18]\n",
    "    arg = ...\n",
    "    return 1.0/(1+np.exp(-arg))\n",
    "\n",
    "\n",
    "\n",
    "theta = np.array([1,1,2])\n",
    "x = np.array([[5,5],\n",
    "              [5,6],\n",
    "              [-5,-5],\n",
    "              [-5,-8]])\n",
    "res = logistic_func(theta, x)\n",
    "# poprosze liste o podanym wymiarze na wyniku\n",
    "assert res.shape == (4,)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARAowqB7F6SP"
   },
   "source": [
    "## Funkcja log-wiarygodności: \n",
    "Parametry regresji znajdujemy przez maksymalizację [funkcji log-wiarygodności](https://brain.fuw.edu.pl/edu/index.php/Uczenie_maszynowe_i_sztuczne_sieci_neuronowe/Wykład_6#Funkcja_wiarygodno.C5.9Bci):\n",
    "\n",
    "$\\qquad$ $l(\\theta) = \\log L(\\theta) = \\sum_{j=1}^m y^{(j)} \\log h(x^{(j)}) + (1 - y^{(j)}) \\log (1 - h(x^{(j)}))$,\n",
    "gdzie:  \n",
    "\n",
    "m - liczebność próbki\n",
    "\n",
    "x - dane wejściowe, u nas wyniki z egaminów z matematyki i biologii\n",
    "\n",
    "y - dane wyjściowe, u nas wynik rekrutacji na studia\n",
    "\n",
    "h - postać zależności wyniku od danych wejściowych. U nas to jest funkcja logistyczna, czyli oczekujemy, że wzór y = h(x) dobrze opisuje zależnośc między danymi wejściowymi, a wyjściowymi.\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "Proszę napisać funkcję ```log_likelihood(theta, x,y, model)``` która:\n",
    "\n",
    "* implementuje funkcję log-wiarygodności\n",
    "* jako argumenty przyjmuje parametry regresji logistycznej  $(\\theta_{0}, \\theta_{1}, ..., \\theta_{i})$ oraz tablicę danych wejściowych $x, y$. \n",
    "* model dla którego szukamy parametrów $\\theta_{i}$ w naszym przypadku to będzie funkcja logistyczna: ```logistic_func```\n",
    "\n",
    "**Uwaga**: argument $theta$ musi być pierwszy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(theta, x, y, model):\n",
    "    # Miejsce na twój kod. Użyj argumentu model zamiast konkretnej funkcji.\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pCk325NF6SS"
   },
   "source": [
    "Maksymalizacja to zadanie optymalizacyjne - szukamy optymalnych parametrów, a kryterium optymalności to maksymalna wartość funckji log-wiarygodności.\n",
    "W tym ćwiczeniu zrobimy to za pomocą funkcji optymalizacyjnych z modułu [<tt>scipy.optimize</tt>]( http://docs.scipy.org/doc/scipy/reference/optimize.html#module-scipy.optimize). \n",
    "\n",
    "\n",
    "Wynikają z tego dwie konsekwencje:\n",
    "* funkcje te są przystosowane do szukania minimów funkcji celu. Musimy więc podawać im jako argumenty funkcję minus log-wiarygodności\n",
    "* niektóre algorytmy mogą działać szybciej jeśli zaimplementujemy jawnie postać pochodnej:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\begin{array}{lcl}\n",
    "\\frac{\\partial}{\\partial \\theta_i} l(\\theta)  =\\sum_{j=1}^m (y^{(j)}-h_\\theta(x^{(j)}))x_i^{(j)}\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proszę napisać funkcję ```negative_log_likelihood(theta, x,y, model)``` która:\n",
    "\n",
    "* zwraca funkcję log-wiarygodności pomnożoną przez $-1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(theta, x, y, model):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proszę napisać funkcję ```log_likelihood_derivative(theta, x,y, model)``` oraz ```negative_log_likelihood_derivative(theta, x,y, model)``` które:\n",
    "\n",
    "* zwraca funkcję pochodną log-wiarygodności\n",
    "* zwraca funkcję pochodną log-wiarygodności pomnożoną przez $-1$\n",
    "\n",
    "**Uwaga**: mnożąc przez $x_{i}$ trzeba uwzględnić kolumnę jedynek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_derivative(theta, x, y, model):\n",
    "    # Miejsce na Twój kod. Pamiętaj, żeby dodać kolumnę jedynek do x.\n",
    "    #\n",
    "    # 1. Policz wynik modelu dzialajacego na danych wejsciowych\n",
    "    ...\n",
    "    # 2. Policz różnicę względem danych wyjściowcyh\n",
    "    ...\n",
    "    # 3. Policz wartość pochodnej pamiętając o sumowaniu\n",
    "    ...\n",
    "    # 4. zwróc wynik, wartość pochodnej we wszystkich kierunkach (kolumnach)\n",
    "    assert result.shape == theta.shape\n",
    "    return result\n",
    "\n",
    "def negative_log_likelihood_derivative(theta, x, y, model):\n",
    "    return -log_likelihood_derivative(theta, x, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgN1eW3HF6Sc"
   },
   "source": [
    "## Procedura minimalizacji funkcji log-wiarygodności ze wsględu na parametry $\\theta$ dla konkretnych danych.\n",
    "\n",
    "W naszym przypadku mamy trzy parametry $\\theta$ - mnożące odpowiednio 1, wynik z matematyki, wynik z biologii.\n",
    "\n",
    "Proszę:\n",
    "* zainicjalizować parametry $\\theta_{0}, \\theta_{1}, \\theta_{2}$ na wartości 0.\n",
    "* obliczyć wartość i pochodną funkcji wiarygodności na danych początkowych \n",
    "\n",
    "Poprawne wartości to:\n",
    "```\n",
    "Wartość funkcji log-wiarygodności dla zbioru testowego = -69.31471805599453\n",
    "Wartość pochodnej funkcji log-wiarygodności dla zbioru testowego = [  10.         1200.92165893 1126.28422055]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = ...\n",
    "model = ...\n",
    "\n",
    "# wartość funkcji log-wiarygodności\n",
    "llh = ...\n",
    "# wartość pochodnej\n",
    "llh_derivative = ...\n",
    "\n",
    "print(\"Wartość funkcji log-wiarygodności dla zbioru testowego = {}\".format(llh))\n",
    "print(\"Wartość pochodnej funkcji log-wiarygodności dla zbioru testowego = {}\".format(llh_derivative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxsFdI1sF6Sv"
   },
   "source": [
    "## Optymalizacja  \n",
    "\n",
    "Funkcje optymalizujące zaczerpniemy z modułu scipy.optimize: ```scipy.optimize.fmin_bfgs```. Ponieważ funkcje te są zaimplementowane do mnimalizowania to zamiast maksymalizować funkcję low-wiarygodności będziemy minimalizować tą funkcje przemnożoną przez -1 czyli ```f=negative_log_likelihood``` oraz ```fprime=negative_log_likelihood_derivative```\n",
    "\n",
    "\n",
    "* proszę wywołać funckję ```scipy.optimize.fmin_bfgs``` z obpowiednimi argumentami.\n",
    "* proszę porównać liczbę wywołań i czas wykonywania komórki z i bez podania explicite postaci pochodnej\n",
    "(https://ipython.readthedocs.io/en/stable/interactive/magics.html?highlight=%25time#cell-magics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ^ rozpoczecie mierzenia czasu dzieje sie tu\n",
    "\n",
    "\n",
    "model = ...\n",
    "\n",
    "# znajdz optymalne parametry theta\n",
    "theta_opt = so.fmin_bfgs(...)\n",
    "# policz log-wiarygodnosc z optymalnymi parametrami\n",
    "llh = log_likelihood(...)\n",
    "\n",
    "print('Optymalne wartości parametrów theta: {}'.format(theta_opt))\n",
    "print(\"Wartość funkcji log-wiarygodności dla optymalnych parametrów: {}\".format(llh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtH3eaZOF6S4"
   },
   "source": [
    "## Wyniki\n",
    "Wyniki regresji logistycznej możemy odbierać na dwa sposoby:\n",
    "* obliczyć wartość hipotezy dla badanego wejścia i dopasowanych parametrów: miara ta ma interpretację prawdopodobieństwa przynależności wejścia do klasy 1,\n",
    "* dopisać funkcję wykonującą klasyfikację, tzn. porównanie wartości hipotezy z 1/2: \n",
    "  * dla wartości hipotezy > 1/2 klasyfikacja zwraca 1, \n",
    "  * w przeciwnym razie 0.\n",
    "  \n",
    "  \n",
    "Proszę napisać funkcję ```classification(theta, x)```  która:\n",
    "* jako argument przyjmuje wektora parametrów modelu $\\theta$, tablicę danych wejściowych $x$, oraz $model$\n",
    "* zwraca listę klasyfikacyjną: $1$ gdy $model(x)>0.5$, a $0$ w przeciwnym przypadku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(theta, x, model):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUmEhM5WF6S8"
   },
   "source": [
    "## Przewidywanie \n",
    "\n",
    "Proszę:\n",
    "\n",
    "* korzystając z modelu ```logistic_func``` wraz z parametrami zwróconymi przez procedurę optymalizacyjną obliczyć prawdopobobieństwo zdania\n",
    "  osoby, która uzyskała 20 pkt. z matematyki, oraz 80 z biologii.\n",
    "* korzystając z funckji \"classification\" wyznaczyć czy osoba należy do   klasy $0$ czy $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stworz dane odpowiadajace naszej osobie\n",
    "x = ...\n",
    "# policz prawdopodobienstwo zdania\n",
    "p = ...\n",
    "\n",
    "print(\"Osoba z {} pkt z matematyki, oraz {} pkt. z biologii ma {}% szans na przyjęcie na studia.\".format(x[0,0], x[0,1], round(p[0]*100,3)))\n",
    "\n",
    "# sklasyfikuj osobę\n",
    "class_number = ...\n",
    "print(\"Osoba zalicza się do klasy: {}\".format(class_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtZFghLDF6TE"
   },
   "source": [
    "Narysujmy uzyskany podział. Na tle punktów pokolorowanych zgodnie z przynależnością do klas dorysujemy prostą rozgraniczającą obszary \"1\" od \"0\".   Ma ona równanie \n",
    "\n",
    "$\\qquad$ $h_\\theta(x)=1/2$, \n",
    "\n",
    "tzn:\n",
    "\n",
    "$\\qquad$ $\\theta x^T = 0$\n",
    "\n",
    "czyli \n",
    "\n",
    "$\\theta_0 +\\theta_1 x_1 + \\theta_2 x_2 =0 $\n",
    "\n",
    "Przekształcając to do równania prostej we współrzędnych $(x_1,x_2)$ mamy:\n",
    "\n",
    "$- \\theta_2 x_2 = \\theta_0 +\\theta_1 x_1 $\n",
    "\n",
    "$ x_2 = - \\frac{1}{\\theta_2}( \\theta_0 +\\theta_1 x_1 )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_passed = df[df[\"wynik\"]==1]\n",
    "df_failed = df[df[\"wynik\"]==0]\n",
    "\n",
    "fig, axes = plt.subplots(1,1, figsize=(5,5))\n",
    "# narysuj tych co zdali na niebiesko, pamietaj o argumencie \"label\" dla czytelnosci\n",
    "...\n",
    "# narysuj tych co nie zdali na czerwono, pamietaj o argumencie \"label\" dla czytelnosci\n",
    "...\n",
    "\n",
    "# znajdz prostą\n",
    "x = ...\n",
    "y = ...\n",
    "\n",
    "# rysowanie prostej i legendy\n",
    "axes.plot(x,y, label=\"logistic regression model\")\n",
    "axes.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8EZX9WNF6TH"
   },
   "source": [
    "## Część II: Walidacja - to na ćwiczenia w przyszłym tygodniu\n",
    "Teoria do tej części znajduje się tu:\n",
    "\n",
    "https://brain.fuw.edu.pl/edu/index.php/Uczenie_maszynowe_i_sztuczne_sieci_neuronowe/Wykład_Ocena_jakości_klasyfikacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzYhR3-IF6TI"
   },
   "source": [
    "### Zastosowanie w naszym przykładzie\n",
    "Dodamy teraz kross-walidację typu $leave-one-out$.\n",
    "Po kolei odłożymy po jednym przykładzie ze zbioru uczącego i na takim zredukownaym zbiorze nauczymy regresję, a następnie sprawdzimy \n",
    "działanie modelu na odłożonym przykładzie:\n",
    "\n",
    "<ol>\n",
    "<li> ze zbioru uczącego odrzucamy jeden przykład </li>\n",
    "<li> na pozostałych przykładach \"trenujemy model\", czyli znajdujemy parametry $\\theta$ </li>\n",
    "<li> sprawdzamy działanie modelu na odrzuconym wcześniej przykładzie</li>\n",
    "<li> procedurę powtarzamy dla wszystkich przykładów w zbiorze uczącym </li>   \n",
    "</ol> \n",
    "\n",
    "Proszę napisać funckję ```leave_one_out_CV(df, theta, model)```\n",
    "\n",
    "która:\n",
    "* przyjmuje zestaw uczący w postaci obiektu DataFrame, początkowych parametrów $\\theta$, oraz model $model$\n",
    "* wykonuje operację \"leave-one-out\" i tworzy listę wyników modelu dla każdego przykładu:\n",
    "\n",
    "```\n",
    "passed = np.append(passed, classification(theta_opt, df_left_out[[\"matematyka\",\"biologia\"]], model))\n",
    "```\n",
    "\n",
    "* dodaje do obiektu DataFrame kolumnę z wynikami modelu:\n",
    "\n",
    "```\n",
    "df[\"model\"] = passed \n",
    "```   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "def leave_one_out_CV(df, theta0, model):\n",
    "    # tutaj bedziemy wpisywac wyniki modelu\n",
    "    passed = np.array([])\n",
    "    # tworzymy kopie data frame\n",
    "    df_with_model = df.copy()\n",
    "    \n",
    "    # pętla po wszystkich przykładach\n",
    "    for leave_out_index in df.index:\n",
    "        # 1. stworz dataframe bez jednego przykladu\n",
    "        df_filtered = ...\n",
    "        # 2. znajdz optymalne parametry theta\n",
    "        theta_opt = ...\n",
    "        # 3. stworz dataframe z odrzuconego (pojedynczego) przykladu\n",
    "        df_left_out = ...\n",
    "        # 4. dodajemy wynik modelu do poprzednich\n",
    "        passed = ...\n",
    "    # Dodajemy wyniki modelu do df_with_model\n",
    "    ...\n",
    "    # zwracamy data frame powiekszony o kolumne z wynikami modelu\n",
    "    return df_with_model\n",
    "                        \n",
    "theta0 = np.array([0,0,0])\n",
    "model = logistic_func \n",
    "df_with_model = leave_one_out_CV(df, theta0, model)\n",
    "print(df_with_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdefiniujmy następujące przypadki gdy nasz model się myli lub podaje poprawny wynik:\n",
    "\n",
    "* \"True Positive\" (TP):  stan faktyczny jest pozytywny (y=1) i klasyfikator się nie myli (wynik = 1)\n",
    "* \"True Negative\" (TN):  stan faktyczny jest negatywny (y=0) i klasyfikator się nie myli (wynik = 0) \n",
    "* \"False Positive\" (FP): wynik fałszywie pozytywny (fałszywy alarm): stan faktyczny jest negatywny (y=0) ale klasyfikator się  myli (wynik = 1)\n",
    "* \"False Netative\" (FN): przegapiony alarm: stan faktyczny jest pozytywny (y=1) i klasyfikator się myli (wynik = 0)\n",
    "\n",
    "Proszę napisać kod, który oblicza TP, TN, FP, FN. Dla naszego zbioru uczącego powinniśmy uzyskać:\n",
    "```\n",
    "TP:  55\n",
    "FP:  6\n",
    "TN:  34\n",
    "FN:  5\n",
    "```   \n",
    "\n",
    "**Wskazówka:** proszę zliczać liczbę wierszy w odpowiednio przefiltrowanym obiekcie DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# podpowiedz: uzyj warunkow logicznych aby wybrac odpowiednie dane i pola shape aby dostac liczbe wierszy\n",
    "tp = ...\n",
    "fp = ...\n",
    "tn = ...\n",
    "fn = ...\n",
    "\n",
    "print(\"TP = {}\\nFP = {}\\nTN = {}\\nFN = {}\".format(tp, fp, tn, fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UNsb51DF6TQ"
   },
   "source": [
    "## Krzywa ROC\n",
    "\n",
    "Aby wykreślić krzywą ROC należy przeprowadzić klasyfikację dla wielu możliwych wartości progu dla hipotezy, powyżej którego uznajemy przypadek za należący do klasy 1. W tym celu należy zmodyfikować funckję ```leave_one_out_CV(df, theta0, model)``` tak by zapisywała prawdopodobieństwo, a nie wynik działania funkcji ```classification```\n",
    "\n",
    "Proszę napisać funkcję ```leave_one_out_CV_with_prob(df, theta0, model)``` \n",
    "* która zapisuje kolumnę z prawdopodobieństwem zamiast wynikiem klasyfikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "def leave_one_out_CV_with_prob(df, theta0, model):\n",
    "    # tutaj bedziemy wpisywac wyniki modelu (prawdopodobienstwa)\n",
    "    prob = np.array([])\n",
    "    # robimy kopie oryginalnych danych do ktorych dodamy kolumne\n",
    "    df_with_model = df.copy()\n",
    "    # petla po wszystkich przykladach\n",
    "    for leave_out_index in df.index:\n",
    "        # 1. stworz dataframe bez jednego przykladu\n",
    "        ...\n",
    "        # 2. znajdz optymalne parametry theta\n",
    "        ...\n",
    "        # 3. stworz dataframe z odrzuconego (pojedynczego) przykladu\n",
    "        ...\n",
    "        # 4. dodajemy wynik modelu do poprzednich\n",
    "        ...\n",
    "    # Dodajemy wyniki modelu (prawdopodobienstwa) do calego data frame\n",
    "    ...\n",
    "    # zwracamy data frame powiekszony o kolumne z wynikami modelu (prawdopodobienstwami)\n",
    "    return df_with_model\n",
    "                        \n",
    "theta0 = np.array([0,0,0])\n",
    "model = logistic_func \n",
    "df_with_prob = leave_one_out_CV_with_prob(df, theta0, model)\n",
    "print(df_with_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korzystając z biblioteki ```sklearn``` proszę narysować krzywą ROC oraz obliczyć pole pod nią (\"area unde ROC, AUC) dla naszego modelu.\n",
    "\n",
    "**Wskazówka:** wpisać w Google hasło \"scikit learn Receiver Operating Characteristic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "fpr, tpr, thresholds = ...\n",
    "roc_auc = ...\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "# narysujmy krzywą (diagonala) dla rzutu monetą (naiwny klasyfikator, najgorsze rozwiązanie)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for logistic regression')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modyfikacja modelu\n",
    "\n",
    "Proszę wykonać trening regresji logistycznej dla modelu, który używa wyniku tylko z jednego egzaminu i narysować na jednym rysunku krzywe ROC dla trzech wariantów:\n",
    "* modelu używającego wyników z obu przedmiotów\n",
    "* modelu używającego tylko wyników z matematyki\n",
    "* modelu używającego tylko wyników z biologii\n",
    "\n",
    "**Wskazówka**: należy przerobić funkcję ```leave_one_out_CV_with_prob``` tak by wykonywała obliczenia dla wszystkich trzech wariantów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def leave_one_out_CV_many_models(df, theta0, model):\n",
    "    # tu zapisujemy wszystkie prawdopodobienstwa\n",
    "    prob = np.array([])\n",
    "    # tu tylko dla matematyki\n",
    "    prob_math = np.array([])\n",
    "    # tu tylko dla biologii\n",
    "    prob_biol = np.array([])\n",
    "    # tu robimy kopie\n",
    "    df_with_model = df.copy()\n",
    "    \n",
    "    for leave_out_index in df.index:\n",
    "        # robimy podobnie jak poprzednio, ale tym razem 3 razy: dla obu dziedzin, tylko dla majcy, tylko dla biologii\n",
    "        ...\n",
    "        ...\n",
    "        ...\n",
    "        ##########\n",
    "    # dodajemy kolumny do data frame i zwracamy\n",
    "    ...\n",
    "    ...\n",
    "    ...\n",
    "    return df_with_model\n",
    "                        \n",
    "theta0 = np.array([0,0,0])\n",
    "model = logistic_func \n",
    "df_with_prob = leave_one_out_CV_many_models(df, theta0, model)\n",
    "print(df_with_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analogicznie jak poprzednio, ale tym razem liczymy i plotujemy 3 ROC \n",
    "fpr, tpr, thresholds = ...\n",
    "fpr_math, tpr_math, thresholds = ...\n",
    "fpr_biol, tpr_biol, thresholds = ...\n",
    "\n",
    "roc_auc = ...\n",
    "roc_auc_math = ...\n",
    "roc_auc_biol = ...\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "# uzupelnij kod poniżej\n",
    "...\n",
    "...\n",
    "...\n",
    "#\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for logistic regression')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zastosowanie do innego rodzaju danych\n",
    "\n",
    "Proszę przeprowadzić procedurę treningu i narysować krzywą ROC dla danych gdzie występuje inny podział między klasami:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nPoints = 100\n",
    "x = 100*np.random.random_sample(nPoints)\n",
    "y = 100*np.random.random_sample(nPoints)\n",
    "\n",
    "df = pd.DataFrame(data=x, columns=[\"matematyka\"])\n",
    "df[\"biologia\"] = y\n",
    "# tworzymy nowe dane\n",
    "df[\"wynik\"] = np.sqrt((x-50)**2 + (y-50)**2)<25\n",
    "# narysuj dwuwymiarowy wykres aby zobaczyc korelacje\n",
    "x = ...\n",
    "x.set_axis_labels('matematyka', 'biologia', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = np.array([0,0,0])\n",
    "model = logistic_func\n",
    "# policz prawdopodobienstwa z modelu przy uzyciu walidacji leave_one_out\n",
    "df_with_prob = ...\n",
    "print(df_with_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# narysuj ŁADNY wykres z krzywą ROC i wypisz AUC w legendzie\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co można zrobić by poprawić działanie modelu na takich danych?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Napisz funkcję logistic_func_1(theta, x) która będzie działać podobnie do oryginalnej, ale dodaj w niej do oryginalnych danych 3 kolumny: kolumnę jedynek (tak jak poprzednio), kolumnę x1^2, kolumnę x2^2, gdzie x1 i x2 to wyniki z matematyki i biologii odpowiednio.\n",
    "\n",
    "Następnie uzupełnij funkcję leave_one_out_CV_with_prob(df, theta0, model), analogicznie do poprzednich funkcji. Funkcja ma wykonać walidację leave-one-out i zwrocić prawdopodbieństwo uzyskane z modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def logistic_func_1(theta, x):\n",
    "    ...\n",
    "\n",
    "def leave_one_out_CV_with_prob(df, theta0, model):\n",
    "    \n",
    "    prob = np.array([])\n",
    "    df_with_model = df.copy()\n",
    "    # napisz petle po przypadkach analoficznie jak poprzednio\n",
    "    ...\n",
    "    # dodaj prob jako nową kolumnę\n",
    "    ...\n",
    "    return df_with_model\n",
    "\n",
    "theta0 = np.array([0,0,0,0,0])\n",
    "model = logistic_func_1 \n",
    "df_with_prob = leave_one_out_CV_with_prob(df, theta0, model)\n",
    "print(df_with_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# narysuj ładny wykres ROC tak jak poprzednio ale dla nowego modelu\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02M_Regresja_logistyczna.ipynb",
   "provenance": [
    {
     "file_id": "1QyygSjtzI9iNile4e8Qlcur7Qn0r_VRN",
     "timestamp": 1546856483810
    }
   ]
  },
  "interpreter": {
   "hash": "fcb4468fb47c6127ab44332c3f3439a85914e2850b2efd86c12e06a03080f93f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "4a92c6b51368406ba79cf23ea6c1be11": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6e5e2c335854c40857d817fde071239": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c276665ac3fb4531948d896f79ef1c22": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a92c6b51368406ba79cf23ea6c1be11",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a6e5e2c335854c40857d817fde071239",
      "value": 100
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
